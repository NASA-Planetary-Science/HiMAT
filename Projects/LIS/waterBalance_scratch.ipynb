{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts to construct a water balance from LIS ouptut, for GRACE comparison and forward modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from himatpy.LIS import utils as LISutils\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/att/pubrepo/hma_data/products/LIS-new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lis_data(data_dir, nc_path, **kwargs)\n",
    "    \"\"\"\n",
    "    This function process LIS Data by breaking it up into yearly netcdf. Only certain variables are exported.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : String.\n",
    "        The location of the Raw LIS NetCDF Data\n",
    "    nc_path : String.\n",
    "        The location of the output NetCDF.\n",
    "    **kwargs: Other keyword arguments that works with get_xr_dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Open all files into a single xarray dataset\n",
    "    ds = get_xr_dataset(data_dir, fname=None, multiple_nc=True, chunks={'time': 1})\n",
    "    desiredds = ds[['Qsm_tavg','Rainf_tavg','Qs_tavg','Snowf_tavg','Qsb_tavg','Evap_tavg','TWS_tavg']]\n",
    "    \n",
    "    dt = pd.DatetimeIndex(desiredds.coords['time'].values)\n",
    "    year_starts = dt[dt.is_year_start]\n",
    "    year_ends = dt[dt.is_year_end]\n",
    "    yearslices = [(x,y) for x,y in zip(year_starts, year_ends)]\n",
    "    \n",
    "    if not os.path.exists(ncpath):\n",
    "        os.mkdir(ncpath)\n",
    "    \n",
    "    bar = progressbar.ProgressBar()\n",
    "    for ys, ye in bar(yearslices):\n",
    "        print('Processing {}...'.format(ys.year))\n",
    "        slicedds = desiredds.sel(time = slice(ys, ye))\n",
    "        procds = slicedds.apply(lambda x: process_da(x))\n",
    "        procds.to_netcdf(os.path.join(ncpath, 'LIS_{}.nc'.format(ys.year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open all files into a single xarray dataset\n",
    "ds = LISutils.get_xr_dataset(data_dir, fname=None, multiple_nc=True, chunks={'time': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:           (SnowIce_profiles: 3, SnowLiq_profiles: 3, SnowTProf_profiles: 3, SoilMoist_profiles: 4, SoilTemp_profiles: 4, east_west: 1896, north_south: 1696, time: 6117, z_soil_profiles: 4)\n",
       "Coordinates:\n",
       "  * time              (time) datetime64[ns] 2001-01-01 2001-01-02 2001-01-03 ...\n",
       "    longitude         (north_south, east_west) float64 66.03 66.04 66.05 ...\n",
       "    latitude          (north_south, east_west) float64 22.02 22.02 22.02 ...\n",
       "Dimensions without coordinates: SnowIce_profiles, SnowLiq_profiles, SnowTProf_profiles, SoilMoist_profiles, SoilTemp_profiles, east_west, north_south, z_soil_profiles\n",
       "Data variables:\n",
       "    lat               (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    lon               (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Swnet_tavg        (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Lwnet_tavg        (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qle_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qh_tavg           (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qg_tavg           (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Snowf_tavg        (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Rainf_tavg        (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Evap_tavg         (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qs_tavg           (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qsb_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qsm_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    VegT_tavg         (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    AvgSurfT_tavg     (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    RadT_tavg         (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Albedo_tavg       (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SWE_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SnowDepth_tavg    (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SnowIce_inst      (time, SnowIce_profiles, north_south, east_west) float64 nan ...\n",
       "    SnowAge_tavg      (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SoilMoist_tavg    (time, SoilMoist_profiles, north_south, east_west) float64 nan ...\n",
       "    SoilTemp_tavg     (time, SoilTemp_profiles, north_south, east_west) float64 nan ...\n",
       "    ECanop_tavg       (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    TVeg_tavg         (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    ESoil_tavg        (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    CanopInt_tavg     (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SubSnow_tavg      (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    TWS_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SnowCover_tavg    (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SnowTProf_inst    (time, SnowTProf_profiles, north_south, east_west) float64 nan ...\n",
       "    Wind_f_tavg       (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Rainf_f_tavg      (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Tair_f_tavg       (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Qair_f_tavg       (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    Psurf_f_tavg      (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    SWdown_f_tavg     (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    LWdown_f_tavg     (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    LAI_tavg          (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    TotalPrecip_tavg  (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    ActSnowNL_inst    (time, north_south, east_west) float64 nan nan nan nan ...\n",
       "    z_soil_inst       (time, z_soil_profiles, north_south, east_west) float64 nan ...\n",
       "    SnowLiq_inst      (time, SnowLiq_profiles, north_south, east_west) float64 nan ...\n",
       "Attributes:\n",
       "    missing_value:           -9999.0\n",
       "    NUM_SOIL_LAYERS:         4\n",
       "    SOIL_LAYER_THICKNESSES:  [ 0.1         0.30000001  0.60000002  1.        ]\n",
       "    title:                   LIS land surface model output\n",
       "    institution:             NASA GSFC\n",
       "    source:                  \n",
       "    history:                 created on date: 2017-11-27T11:54:06.232\n",
       "    references:              Kumar_etal_EMS_2006, Peters-Lidard_etal_ISSE_2007\n",
       "    conventions:             CF-1.6\n",
       "    comment:                 website: http://lis.gsfc.nasa.gov/\n",
       "    MAP_PROJECTION:          EQUIDISTANT CYLINDRICAL\n",
       "    SOUTH_WEST_CORNER_LAT:   22.025\n",
       "    SOUTH_WEST_CORNER_LON:   66.025\n",
       "    DX:                      0.01\n",
       "    DY:                      0.01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desiredds = ds[['Qsm_tavg','Rainf_tavg','Qs_tavg','Snowf_tavg','Qsb_tavg','Evap_tavg','TWS_tavg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dt = pd.DatetimeIndex(desiredds.coords['time'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_starts = dt[dt.is_year_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ends = dt[dt.is_year_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearslices = [(x,y) for x,y in zip(year_starts, year_ends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Timestamp('2001-01-01 00:00:00'), Timestamp('2001-12-31 00:00:00')),\n",
       " (Timestamp('2002-01-01 00:00:00'), Timestamp('2002-12-31 00:00:00')),\n",
       " (Timestamp('2003-01-01 00:00:00'), Timestamp('2003-12-31 00:00:00')),\n",
       " (Timestamp('2004-01-01 00:00:00'), Timestamp('2004-12-31 00:00:00')),\n",
       " (Timestamp('2005-01-01 00:00:00'), Timestamp('2005-12-31 00:00:00')),\n",
       " (Timestamp('2006-01-01 00:00:00'), Timestamp('2006-12-31 00:00:00')),\n",
       " (Timestamp('2007-01-01 00:00:00'), Timestamp('2007-12-31 00:00:00')),\n",
       " (Timestamp('2008-01-01 00:00:00'), Timestamp('2008-12-31 00:00:00')),\n",
       " (Timestamp('2009-01-01 00:00:00'), Timestamp('2009-12-31 00:00:00')),\n",
       " (Timestamp('2010-01-01 00:00:00'), Timestamp('2010-12-31 00:00:00')),\n",
       " (Timestamp('2011-01-01 00:00:00'), Timestamp('2011-12-31 00:00:00')),\n",
       " (Timestamp('2012-01-01 00:00:00'), Timestamp('2012-12-31 00:00:00')),\n",
       " (Timestamp('2013-01-01 00:00:00'), Timestamp('2013-12-31 00:00:00')),\n",
       " (Timestamp('2014-01-01 00:00:00'), Timestamp('2014-12-31 00:00:00')),\n",
       " (Timestamp('2015-01-01 00:00:00'), Timestamp('2015-12-31 00:00:00')),\n",
       " (Timestamp('2016-01-01 00:00:00'), Timestamp('2016-12-31 00:00:00'))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearslices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/api.py\", line 573, in to_netcdf\n",
      "    unlimited_dims=unlimited_dims)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/core/dataset.py\", line 918, in dump_to_store\n",
      "    store.sync()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\", line 336, in sync\n",
      "    super(NetCDF4DataStore, self).sync()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/common.py\", line 202, in sync\n",
      "    self.writer.sync()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/common.py\", line 179, in sync\n",
      "    da.store(self.sources, self.targets, lock=GLOBAL_LOCK)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/dask/array/core.py\", line 900, in store\n",
      "    Array._get(dsk, keys, **kwargs)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/dask/base.py\", line 106, in _get\n",
      "    return get(dsk2, keys, **kwargs)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/dask/threaded.py\", line 75, in get\n",
      "    pack_exception=pack_exception, **kwargs)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/dask/local.py\", line 512, in get_async\n",
      "    key, res_info, failed = queue_get(queue)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/dask/local.py\", line 151, in queue_get\n",
      "    return q.get()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/queue.py\", line 164, in get\n",
      "    self.not_empty.wait()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/threading.py\", line 295, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-60-92b1694a0369>\", line 7, in <module>\n",
      "    procds.to_netcdf(os.path.join(ncpath, 'LIS_{}.nc'.format(ys.year)))\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/core/dataset.py\", line 977, in to_netcdf\n",
      "    unlimited_dims=unlimited_dims)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/api.py\", line 578, in to_netcdf\n",
      "    store.close()\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/xarray/backends/netCDF4_.py\", line 344, in close\n",
      "    ds.close()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1828, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/lsetiawa/miniconda/envs/himat/lib/python3.6/inspect.py\", line 730, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ncpath = '/att/nobackup/lsetiawa/LISYear/'\n",
    "if not os.path.exists(ncpath):\n",
    "    os.mkdir(ncpath)\n",
    "for ys, ye in yearslices:\n",
    "    slicedds = desiredds.sel(time = slice(ys, ye))\n",
    "    procds = slicedds.apply(lambda x: process_da(x))\n",
    "    procds.to_netcdf(os.path.join(ncpath, 'LIS_{}.nc'.format(ys.year)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicedds = desiredds.sel(time = slice(yearslices[0][0], yearslices[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:     (east_west: 1896, north_south: 1696, time: 365)\n",
       "Coordinates:\n",
       "  * time        (time) datetime64[ns] 2001-01-01 2001-01-02 2001-01-03 ...\n",
       "    latitude    (north_south, east_west) float64 22.02 22.02 22.02 22.02 ...\n",
       "    longitude   (north_south, east_west) float64 66.03 66.04 66.05 66.06 ...\n",
       "Dimensions without coordinates: east_west, north_south\n",
       "Data variables:\n",
       "    Qsm_tavg    (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    Rainf_tavg  (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    Qs_tavg     (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    Snowf_tavg  (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    Qsb_tavg    (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    Evap_tavg   (time, north_south, east_west) float64 nan nan nan nan nan ...\n",
       "    TWS_tavg    (time, north_south, east_west) float64 nan nan nan nan nan ..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicedds.apply(lambda x: process_da(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_da(da):\n",
    "    # attributes for the first 6 variables:\n",
    "    text = 'Daily {variable} in units of mm we'.format\n",
    "\n",
    "    # attributes for the TWS:\n",
    "    \n",
    "    new_attrs = OrderedDict()\n",
    "    for k, v in da.attrs.items():\n",
    "        new_attrs.update({k:v})\n",
    "    new_attrs.update({'units': 'mm we'})\n",
    "    \n",
    "    if da.attrs['standard_name'] == 'terrestrial_water_storage':\n",
    "        new_attrs.update({'long_name': 'Daily change in water storage'})\n",
    "        multda = da\n",
    "    else:\n",
    "        new_attrs.update({'long_name': text(variable=da.attrs['standard_name'])})\n",
    "        multda = da * 864000\n",
    "    \n",
    "    multda.attrs = new_attrs\n",
    "    \n",
    "    return multda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_monthly_avg(ds, des_vars, export_nc=False, out_pth=None):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NEXT calculate monthly sums, e.g.:\n",
    "\n",
    "with ProgressBar():\n",
    "    TWS_monthly = ds['TWS_tavg'].resample('MS', 'time', how = 'sum')\n",
    "\n",
    "# attributes for the first 6 variables:\n",
    "\n",
    "{'Cumulative monthly <snowmelt> in units of mm we','Cumulative monthly ....'}\n",
    "\n",
    "# attributes for TWS:\n",
    "\n",
    "{'Change in monthly water storage'}\n",
    "       \n",
    "# then concatenate all into a single netcdf and serialize to /att/nobackup/aarendt    \n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
